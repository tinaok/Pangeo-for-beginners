{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [

    "# DASK on HPC with PBS Pro batch queue system.\n",
    "\n",
    "To use [pangeo](http://pangeo.io), one needs to get familiar with key python packages of its ecosystem.\n",
    "[Dask](http://dask.org) is one of them which allow you to distribute your computations.\n",
    "We'll focus here on distribution over an HPC cluster equipped with PBS Pro.\n",
    "The distribution requires spawning a dask distributed cluster which will be achieved with [Dask_Jobqueue](https://dask-jobqueue.readthedocs.io/en/latest/).\n",
    "\n",
    "Other methods of HPC deployments are possible and more ressources about these can be found on [pangeo.io](http://pangeo.io/setup_guides/hpc.html)\n",
    "\n",
    "***For MPI users, this resembles the combination of writing code using mpi_init and submitting mpirun script to a cluster (without knowing what your context of code will be....)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Set up python environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import dask\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Set up a dask distributed cluster  \n",
    "This configuration is based on hal (CNES HPC Cluster) which use PBSPro as batch scheduler.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "cluster = PBSCluster(cores=6, memory='30 gb', walltime='1:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Above request 6 cores with 30GB of memory, walltime as 1 hours for each dask client.  In case you need small chunk of nodes, one can modify, for example 5 Gb of memory on each node, as below.:\n",
    "\n",
    "\n",
    "`cluster = PBSCluster(cores=1,memory='5 gb', walltime='1:00:00')`\n",
    "\n",
    "ATT these chunks should be chosen well so that the chunk fits well to the cluster's pbs configuration. In this example it chose 'walltime 1 hour' since that is the max time limit of short quick job queue.\n",
    "\n",
    "\n",
    "**If you make short, and small chunk, your job generally fits gaps of unused resources of the HPC machine and may thus start faster.\n",
    "But this also creates small chunks of used resources, which makes it difficult to run big job for other users.\n",
    "Thus it is important to interact with your HPC cluster managers about your setup.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.  Spawn DASK workers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = cluster.scale(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning: after execution of the cell above, we are effectively starting to 'occupy' resources on the HPC cluster**\n",
    "\n",
    "**Hence, do not forget to kill your 'dask-cluster' using the command 'qdel' in a shell, or the command 'cluster.close()' in the notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Check status of batch jobs\n",
    "\n",
    "You can do this in a shell with the following command and see if your pbs jobs are running or not, and, if running, on which nodes.\n",
    "You can try to connect these nodes with ssh, and check the status of Dask workers with 'top, ps ...' commands.\n",
    "\n",
    "qstat -u your-login -n -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "admin01: \n",
      "                                                            Req'd  Req'd   Elap\n",
      "Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n",
      "--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n",
      "4398868.admin01 odakat   qdev     jupyterhub  72639   1  16   61gb 12:00 R 05:20 node558/0*16\n",
      "4410568.admin01 odakat   qt1h     dask-worke   9146   1   6   28gb 01:00 R 00:00 node089/3*6\n",
      "4410569.admin01 odakat   qt1h     dask-worke  22172   1   6   28gb 01:00 R 00:00 node098/1*6\n",
      "4410570.admin01 odakat   qt1h     dask-worke  22174   1   6   28gb 01:00 R 00:00 node098/2*6\n",
      "4410571.admin01 odakat   qt1h     dask-worke  23137   1   6   28gb 01:00 R 00:00 node099/1*6\n",
      "4410572.admin01 odakat   qt1h     dask-worke  23152   1   6   28gb 01:00 R 00:00 node099/2*6\n",
      "4410573.admin01 odakat   qt1h     dask-worke   4493   1   6   28gb 01:00 R 00:00 node104/1*6\n",
      "4410574.admin01 odakat   qt1h     dask-worke  17044   1   6   28gb 01:00 R 00:00 node107/1*6\n",
      "4410575.admin01 odakat   qt1h     dask-worke  17579   1   6   28gb 01:00 R 00:00 node114/1*6\n",
      "4410576.admin01 odakat   qt1h     dask-worke   2706   1   6   28gb 01:00 R 00:00 node115/2*6\n",
      "4410577.admin01 odakat   qt1h     dask-worke  17619   1   6   28gb 01:00 R 00:00 node137/1*6\n"
     ]
    }
   ],
   "source": [
    "!qstat -u odakat -n -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe following command can be used to check the actual pbs scripts that have been submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#!/usr/bin/env bash\n",
      "#PBS -N dask-worker\n",
      "#PBS -l select=1:ncpus=6:mem=28GB\n",
      "#PBS -l walltime=1:00:00\n",
      "JOB_ID=${PBS_JOBID%.*}\n",
      "\n",
      "\n",
      "\n",
      "/home/mp/odakat/miniconda3/envs/equinox/bin/python -m distributed.cli.dask_worker tcp://10.120.43.58:57642 --nthreads 6 --memory-limit 30.00GB --name dask-worker--${JOB_ID}-- --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    " print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following commands will instantiate the Dask client and will enable you to check your Dask client status among other things.\n",
    "The ***Dashboard*** link enable you to monitor your Dask cluster.\n",
    "- Dask dashbord's worker tab to see how each worker use memory and cpu in a graphical mode.  \n",
    "- 'System' shows system usage of jupyternotebook which host DASK scheduler.  \n",
    "Other tabs are also usefull to understand how parallel process are working.  ATT, it use cpu and memory of your 'jupyter notebook node' if you try to see too complicated graphical interface, your jupyter notebook itself may get slower.\n",
    "\n",
    "**AP: I am not sure ATT is a standard acronym**\n",
    "\n",
    "See the [distributed doc](http://distributed.dask.org/en/latest/web.html) for a video walkthrough of the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.120.43.58:57642\n",
       "  <li><b>Dashboard: </b><a href='http://10.120.43.58:8787/status' target='_blank'>http://10.120.43.58:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>10</li>\n",
       "  <li><b>Cores: </b>60</li>\n",
       "  <li><b>Memory: </b>300.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.120.43.58:57642' processes=10 cores=60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client=Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Increase  (or scale down) number of workers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale_up(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "admin01: \n",
      "                                                            Req'd  Req'd   Elap\n",
      "Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n",
      "--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n",
      "4398868.admin01 odakat   qdev     jupyterhub  72639   1  16   61gb 12:00 R 05:20\n",
      "4410568.admin01 odakat   qt1h     dask-worke   9146   1   6   28gb 01:00 R 00:00\n",
      "4410569.admin01 odakat   qt1h     dask-worke  22172   1   6   28gb 01:00 R 00:00\n",
      "4410570.admin01 odakat   qt1h     dask-worke  22174   1   6   28gb 01:00 R 00:00\n",
      "4410571.admin01 odakat   qt1h     dask-worke  23137   1   6   28gb 01:00 R 00:00\n",
      "4410572.admin01 odakat   qt1h     dask-worke  23152   1   6   28gb 01:00 R 00:00\n",
      "4410573.admin01 odakat   qt1h     dask-worke   4493   1   6   28gb 01:00 R 00:00\n",
      "4410574.admin01 odakat   qt1h     dask-worke  17044   1   6   28gb 01:00 R 00:00\n",
      "4410575.admin01 odakat   qt1h     dask-worke  17579   1   6   28gb 01:00 R 00:00\n",
      "4410576.admin01 odakat   qt1h     dask-worke   2706   1   6   28gb 01:00 R 00:00\n",
      "4410577.admin01 odakat   qt1h     dask-worke  17619   1   6   28gb 01:00 R 00:00\n",
      "4410591.admin01 odakat   qt1h     dask-worke  27124   1   6   28gb 01:00 R 00:00\n"
     ]
    }
   ],
   "source": [
    "!qstat -u odakat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "!qstat -u odakat |grep dask-work |wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of Dask workers increased; you have 11 of them now. One could use other commands like\n",
    "`cluster.scale(11)` instead of `cluster.scale_up(11)`.\n",
    "\n",
    "You can also try to decrease the number of workers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "admin01: \n",
      "                                                            Req'd  Req'd   Elap\n",
      "Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n",
      "--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n",
      "4398868.admin01 odakat   qdev     jupyterhub  72639   1  16   61gb 12:00 R 05:20\n",
      "4410573.admin01 odakat   qt1h     dask-worke   4493   1   6   28gb 01:00 R 00:00\n",
      "4410574.admin01 odakat   qt1h     dask-worke  17044   1   6   28gb 01:00 R 00:00\n",
      "4410575.admin01 odakat   qt1h     dask-worke  17579   1   6   28gb 01:00 R 00:00\n",
      "4410576.admin01 odakat   qt1h     dask-worke   2706   1   6   28gb 01:00 R 00:00\n",
      "4410577.admin01 odakat   qt1h     dask-worke  17619   1   6   28gb 01:00 R 00:00\n"
     ]
    }
   ],
   "source": [
    "!qstat -u odakat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say your dask-worker been killed for whatever the reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat -u odakat |grep dask-work |awk '{print \"qdel \" $1 }' >./del-daskworker \n",
    "!chmod +x ./del-daskworker\n",
    "!./del-daskworker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.120.43.58:57642\n",
       "  <li><b>Dashboard: </b><a href='http://10.120.43.58:8787/status' target='_blank'>http://10.120.43.58:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.120.43.58:57642' processes=0 cores=0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can re-scale your cluster and get back to your parallel computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "admin01: \n",
      "                                                            Req'd  Req'd   Elap\n",
      "Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n",
      "--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n",
      "4398868.admin01 odakat   qdev     jupyterhub  72639   1  16   61gb 12:00 R 05:21\n",
      "4410601.admin01 odakat   qt1h     dask-worke   9444   1   6   28gb 01:00 R 00:00\n",
      "4410602.admin01 odakat   qt1h     dask-worke  22503   1   6   28gb 01:00 R 00:00\n",
      "4410603.admin01 odakat   qt1h     dask-worke  22505   1   6   28gb 01:00 R 00:00\n",
      "4410604.admin01 odakat   qt1h     dask-worke  23454   1   6   28gb 01:00 R 00:00\n",
      "4410605.admin01 odakat   qt1h     dask-worke  23455   1   6   28gb 01:00 R 00:00\n",
      "4410606.admin01 odakat   qt1h     dask-worke   4765   1   6   28gb 01:00 R 00:00\n",
      "4410607.admin01 odakat   qt1h     dask-worke  17752   1   6   28gb 01:00 R 00:00\n",
      "4410608.admin01 odakat   qt1h     dask-worke  18295   1   6   28gb 01:00 R 00:00\n"
     ]
    }
   ],
   "source": [
    "!qstat -u odakat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. once you're done computing, do not forget to stop your dask worker with following command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
