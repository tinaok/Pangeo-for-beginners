{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. What is zarr? \n",
    "\n",
    "**zarr is a file format for storing big data.\n",
    "It divides your big data  into 'chunks' sized files and store each chunks in individual files located in your zarr archive (a simple directory).\n",
    "The 'zarr' archive also stores dimensions, coordinates, and, attributes, e.g. all sorts of meta data.**\n",
    "\n",
    "Long story short for netcdf users: it cuts your netcdf file in small pieces and store them in a directory which ease parallelism. \n",
    "Without using MPI-IO, you can write/read your zarr file in parallel.\n",
    "For netcdf file instead, even if you use hdf5/parallel format, if you do not read/write your file from MPI job, you cannot read/write your file in parallel. Netcdf is not thread safe.\n",
    "\n",
    "**AP: try to simplify the last sentence maybe**\n",
    "\n",
    "For oceanographic numerical modelers: if you chose to read/write data as zarr (not in netcdf) online, for each mpi domain that you compute with a mpi process, it just writes your 'zarr' chunk, that may speed up overall computation time by winning IO access...\n",
    "\n",
    "**AP: idem, try to simplify the last sentence maybe**\n",
    "\n",
    "Take away tips for users that are on Lustre filesystem (**AP: need to ask admin, or available command ?**):\n",
    "As zarr is composed of many small files, do not forget to change the striping as 1 for the directory you use, before you starts to store your zarr file.\n",
    " `mkdir dir_for_zarr`\n",
    " `lfs setstripe -c 1 dir_for_zarr `\n",
    "Then, save your zarr file in 'dir_for_zarr'\n",
    "\n",
    "Link to zarr documentation https://zarr.readthedocs.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.  Set up your python environment \n",
    "call python environments to use xarray and Dask, then create a Dask cluster (as explained in notebook [1 DASK, with HPC cluster](https://github.com/tinaok/Pangeo-for-beginners/blob/master/1%20DASK%2C%20with%20HPC%20cluster%20(PBS%20Pro).ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "cluster = PBSCluster(cores=6,memory='30 gb', walltime='1:00:00')\n",
    "w = cluster.scale(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.120.43.58:59577\n",
       "  <li><b>Dashboard: </b><a href='http://10.120.43.58:8787/status' target='_blank'>http://10.120.43.58:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>10</li>\n",
       "  <li><b>Cores: </b>60</li>\n",
       "  <li><b>Memory: </b>300.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.120.43.58:59577' processes=10 cores=60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client=Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Read a zarr file, as xarray data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='/work/ALT/swot/swotpub/LLC4320/zarr/SST.zarr'\n",
    "ds =xr.open_zarr(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (face: 13, i: 4320, j: 4320, time: 8785)\n",
      "Coordinates:\n",
      "    dtime    (time) datetime64[ns] dask.array<shape=(8785,), chunksize=(8785,)>\n",
      "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "  * i        (i) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "    iters    (time) int64 dask.array<shape=(8785,), chunksize=(1,)>\n",
      "  * j        (j) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * time     (time) float64 5.702e+06 5.706e+06 5.71e+06 ... 3.732e+07 3.732e+07\n",
      "Data variables:\n",
      "    SST      (time, face, j, i) float32 dask.array<shape=(8785, 13, 4320, 4320), chunksize=(1, 1, 4320, 4320)>\n",
      "\n",
      " data size: 8525.4 GB\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "print('\\n data size: %.1f GB' %(ds.nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you see that the zarr archive contains a dataset of 'SST' (sea surface temperature) that has several dimensions: face, i, j , and, time.\n",
    "- dtime is a time cordinate.\n",
    "- i and j dimensions correspond approximately to longitudes and latitudes (XC and YC), \n",
    "- face corresponds to one of 13 patch of earth surface. \n",
    "\n",
    "---\n",
    "\n",
    "'chunksize=(1, 1, 4320, 4320)' indicates that size of each chunks SST is divided into. \n",
    "This is like a mpi domain decomposition.\n",
    "When needed,  Dask workers will load some of these chunks in memory and make computations.  \n",
    "With zarr, the zarr archive directory contains as many files as there are chunks.\n",
    "\n",
    "If you use parallel file system, with huge data size (**AP: you need numbers here, >1TB for example**), one should have less meta-data access.\n",
    "Thus you better put enough size of data in each chunk, otherwise you'll just kill the meta-data server.  \n",
    "But, if you want to put your data in the cash of disk to have fast read-wrtite of your data, this chunk size should be smaller than the cash size, so that controller considers that these are the 'cachable' small files. (like GPFS...)  \n",
    "In anycase, before you \n",
    "\n",
    "**AP: last paragraph need rewriting and clarification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Look into a zarr archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zarr archive `SST.zarr` is a directory. \n",
    "It contains directories that corresponds to 'coordinates' (dtime, face, i iters, j, time), and data variables (SST).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  dtime  face  i  iters  j  SST  time  .zattrs  .zgroup\n"
     ]
    }
   ],
   "source": [
    "!ls -a /work/ALT/swot/swotpub/LLC4320/zarr/SST.zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data variable directory 'SST' contains 114205 file, because 8785 (time) * 13(face) =114205, and each file corresponds to the data in each 'chunk'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114205\n"
     ]
    }
   ],
   "source": [
    "!ls -1 /work/ALT/swot/swotpub/LLC4320/zarr/SST.zarr/SST/ |wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.0.0\n",
      "0.1.0.0\n",
      "0.10.0.0\n",
      "0.11.0.0\n",
      "0.12.0.0\n",
      "0.2.0.0\n",
      "0.3.0.0\n",
      "0.4.0.0\n",
      "0.5.0.0\n",
      "0.6.0.0\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!ls -1 /work/ALT/swot/swotpub/LLC4320/zarr/SST.zarr/SST/ |head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, 0.1.0.0 contains, SST data for time = 0, face=1, and  i= 0-4319, and j= 0- 4319 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  0  .zarray  .zattrs\n"
     ]
    }
   ],
   "source": [
    "!ls -a /work/ALT/swot/swotpub/LLC4320/zarr/SST.zarr/dtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chunks\": [\n",
      "        8785\n",
      "    ],\n",
      "    \"compressor\": {\n",
      "        \"blocksize\": 0,\n",
      "        \"clevel\": 5,\n",
      "        \"cname\": \"lz4\",\n",
      "        \"id\": \"blosc\",\n",
      "        \"shuffle\": 1\n",
      "    },\n",
      "    \"dtype\": \"<i8\",\n",
      "    \"fill_value\": null,\n",
      "    \"filters\": null,\n",
      "    \"order\": \"C\",\n",
      "    \"shape\": [\n",
      "        8785\n",
      "    ],\n",
      "    \"zarr_format\": 2\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat /work/ALT/swot/swotpub/LLC4320/zarr/SST.zarr/dtime/.zarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look into the `.zarray` file to see the zarr files encodings. \n",
    "You can also access to encoding with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunks': (1, 1, 4320, 4320),\n",
       " 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0),\n",
       " 'filters': None,\n",
       " '_FillValue': nan,\n",
       " 'dtype': dtype('float32'),\n",
       " 'coordinates': 'dtime iters'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.SST.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunks': (8785,),\n",
       " 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0),\n",
       " 'filters': None,\n",
       " 'units': 'hours since 2011-11-15 00:00:00',\n",
       " 'calendar': 'proleptic_gregorian',\n",
       " 'dtype': dtype('int64')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.dtime.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunks': (1,),\n",
       " 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0),\n",
       " 'filters': None,\n",
       " 'dtype': dtype('int64')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.iters.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4.  Let's try to create a subset of data we just read, and write to another zarr file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (face: 13, i: 4320, j: 4320, time: 1000)\n",
      "Coordinates:\n",
      "    dtime    (time) datetime64[ns] dask.array<shape=(1000,), chunksize=(1000,)>\n",
      "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "  * i        (i) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "    iters    (time) int64 dask.array<shape=(1000,), chunksize=(1,)>\n",
      "  * j        (j) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * time     (time) float64 5.702e+06 5.706e+06 5.71e+06 ... 9.295e+06 9.299e+06\n",
      "Data variables:\n",
      "    SST      (time, face, j, i) float32 dask.array<shape=(1000, 13, 4320, 4320), chunksize=(1, 1, 4320, 4320)>\n",
      "\n",
      " data size: 970.4 GB\n"
     ]
    }
   ],
   "source": [
    "dsmille=ds.isel(time=slice(0,1000))\n",
    "print(dsmille)\n",
    "print('\\n data size: %.1f GB' %(dsmille.nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 5.43 s, total: 1min 45s\n",
      "Wall time: 10min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x2b77370e6da0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dsmille.to_zarr('/work/scratch/odakat/test.zarr', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338G\t/work/scratch/odakat/test.zarr\n"
     ]
    }
   ],
   "source": [
    "!du -hs /work/scratch/odakat/test.zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the dataset selected is 970G big in memory.\n",
    "Compressions is used for the zarr archive(heritated from 'compressor 'encoding from orignal file 'ds' which is , \n",
    "`'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0)`\n",
    "so the filesize it self is 338G.\n",
    "\n",
    "**AP: I am not sure compression is heritated here but it's simply the default ...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. We can try other compression method here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blosclz', 'lz4', 'lz4hc', 'snappy', 'zlib', 'zstd']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numcodecs import blosc\n",
    "blosc.list_compressors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "compressor = zarr.Blosc(cname='zstd', clevel=3, shuffle=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 22s, sys: 8.16 s, total: 3min 30s\n",
      "Wall time: 11min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x2b7741d79630>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dsmille.to_zarr('/work/scratch/odakat/testzarr',  encoding={'SST': {'compressor': compressor}} , mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319G\t/work/scratch/odakat/testzarr\n"
     ]
    }
   ],
   "source": [
    "!du -hs /work/scratch/odakat/testzarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Well, looks like new compressing made us win about 20 G of space, but took 1.5 min more.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "## 6. clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /work/scratch/odakat/testzarr\n",
    "!rm -rf /work/scratch/odakat/test.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
