{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AP: is this a temporary or work in progress notebook? If yes, maybe find another name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading a zarr file, I re-chunk the data using xarray.Dataset.chunk. \n",
    "Then create a newly chunked data stored as zarr file with xarray.Dataset.to_zarr\n",
    "But I get error message: 'notimplemented error \n",
    "my xarray version is12.1, \n",
    "This looks like the bug which is already fixed at https://github.com/pydata/xarray/issues/2300 ...Why do I get 'notimplemented error ?\n",
    "Do I have to use 'del dsread.data.encoding['chunks']. each time before using 'Dataset.to_zarr' as a workaround? or may be I am missing somthing.  I hope someone can help me.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. setting up enviroments,  create a xarray.Dataset and save as zarr file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.1\n"
     ]
    }
   ],
   "source": [
    "print(xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 200, lon: 100, time: 10000)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 0.0 0.2462 0.4925 0.7387 ... 48.26 48.51 48.75 49.0\n",
      "  * lon      (lon) float64 0.0 0.4949 0.9899 1.485 ... 47.52 48.01 48.51 49.0\n",
      "  * time     (time) datetime64[ns] 2001-01-01 ... 2002-02-21T15:00:00\n",
      "Data variables:\n",
      "    data     (lat, lon, time) float64 0.1008 0.5696 0.4781 ... 0.4254 0.3909\n",
      "\n",
      " data size: 1.6 GB\n"
     ]
    }
   ],
   "source": [
    "data = np.random.rand(200,100,10000)\n",
    "#Dimensions  = ['lat','lon','time']\n",
    "times = pd.date_range('2001-01-01',periods=10000,freq='H')\n",
    "lon= np.linspace(0,49,100)\n",
    "lat = np.linspace(0,49,200)\n",
    "#lat = np.arange(-5,5,0.1)\n",
    "da = xr.DataArray(data, coords=[lat,lon,times], dims=['lat','lon','time'])\n",
    "ds = xr.Dataset({'data':da})\n",
    "print(ds)\n",
    "print('\\n data size: %.1f GB' %(ds.nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (lat: 200, lon: 100, time: 10000)\n",
       "Coordinates:\n",
       "  * lat      (lat) float64 0.0 0.2462 0.4925 0.7387 ... 48.26 48.51 48.75 49.0\n",
       "  * lon      (lon) float64 0.0 0.4949 0.9899 1.485 ... 47.52 48.01 48.51 49.0\n",
       "  * time     (time) datetime64[ns] 2001-01-01 ... 2002-02-21T15:00:00\n",
       "Data variables:\n",
       "    data     (lat, lon, time) float64 dask.array<shape=(200, 100, 10000), chunksize=(200, 100, 1)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds =  ds.chunk({'lat': -1, 'lon': -1, 'time': 1})\n",
    "xr.DataArray(data)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 5.74 s, total: 17.2 s\n",
      "Wall time: 8.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x3733db6a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time ds.to_zarr('ds.zarr',  mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.read the zarr file and re-chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'128MiB'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask.config.get('array.chunk-size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 200, lon: 100, time: 10000)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 0.0 0.2462 0.4925 0.7387 ... 48.26 48.51 48.75 49.0\n",
      "  * lon      (lon) float64 0.0 0.4949 0.9899 1.485 ... 47.52 48.01 48.51 49.0\n",
      "  * time     (time) datetime64[ns] 2001-01-01 ... 2002-02-21T15:00:00\n",
      "Data variables:\n",
      "    data     (lat, lon, time) float64 dask.array<shape=(200, 100, 10000), chunksize=(50, 25, 10000)>\n",
      "\n",
      " data size: 1.6 GB\n"
     ]
    }
   ],
   "source": [
    "dstmp =xr.open_zarr('./ds.zarr')\n",
    "dsread =  dstmp.chunk({'lat': 'auto', 'lon': 'auto', 'time': -1})\n",
    "print(dsread)\n",
    "print('\\n data size: %.1f GB' %(dsread.nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. write the re-chunked data to a file, then err message as following.\n",
    "'NotImplementedError: Specified zarr chunks (200, 100, 1) would overlap multiple dask chunks ((50, 50, 50, 50), (25, 25, 25, 25), (10000,)). This is not implemented in xarray yet.  Consider rechunking the data using `chunk()` or specifying different chunks in encoding.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Specified zarr chunks (200, 100, 1) would overlap multiple dask chunks ((50, 50, 50, 50), (25, 25, 25, 25), (10000,)). This is not implemented in xarray yet.  Consider rechunking the data using `chunk()` or specifying different chunks in encoding.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mit_equinox/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mto_zarr\u001b[0;34m(self, store, mode, synchronizer, group, encoding, compute, consolidated)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         return to_zarr(self, store=store, mode=mode, synchronizer=synchronizer,\n\u001b[1;32m   1367\u001b[0m                        \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m                        consolidated=consolidated)\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mit_equinox/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mto_zarr\u001b[0;34m(dataset, store, mode, synchronizer, group, encoding, compute, consolidated)\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArrayWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;31m# TODO: figure out how to properly handle unlimited_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m     \u001b[0mdump_to_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m     \u001b[0mwrites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mit_equinox/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mdump_to_store\u001b[0;34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     store.store(variables, attrs, check_encoding, writer,\n\u001b[0;32m--> 851\u001b[0;31m                 unlimited_dims=unlimited_dims)\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mit_equinox/lib/python3.6/site-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36mstore\u001b[0;34m(self, variables, attributes, *args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         AbstractWritableDataStore.store(self, variables, attributes,\n\u001b[0;32m--> 344\u001b[0;31m                                         *args, **kwargs)\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mit_equinox/lib/python3.6/site-packages/xarray/backends/common.py\u001b[0m in \u001b[0;36mstore\u001b[0;34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         self.set_variables(variables, check_encoding_set, writer,\n\u001b[0;32m--> 265\u001b[0;31m                            unlimited_dims=unlimited_dims)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mit_equinox/lib/python3.6/site-packages/xarray/backends/common.py\u001b[0m in \u001b[0;36mset_variables\u001b[0;34m(self, variables, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck_encoding_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             target, source = self.prepare_variable(\n\u001b[0;32m--> 303\u001b[0;31m                 name, v, check, unlimited_dims=unlimited_dims)\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mit_equinox/lib/python3.6/site-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36mprepare_variable\u001b[0;34m(self, name, variable, check_encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         encoding = _extract_zarr_variable_encoding(\n\u001b[0;32m--> 328\u001b[0;31m             variable, raise_on_invalid=check_encoding)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mencoded_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mit_equinox/lib/python3.6/site-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36m_extract_zarr_variable_encoding\u001b[0;34m(variable, raise_on_invalid)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     chunks = _determine_zarr_chunks(encoding.get('chunks'), variable.chunks,\n\u001b[0;32m--> 181\u001b[0;31m                                     variable.ndim)\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chunks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mit_equinox/lib/python3.6/site-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36m_determine_zarr_chunks\u001b[0;34m(enc_chunks, var_chunks, ndim)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m\" Consider rechunking the data using \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0;34m\"`chunk()` or specifying different chunks in encoding.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         % (enc_chunks_tuple, var_chunks))\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mzchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 raise ValueError(\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Specified zarr chunks (200, 100, 1) would overlap multiple dask chunks ((50, 50, 50, 50), (25, 25, 25, 25), (10000,)). This is not implemented in xarray yet.  Consider rechunking the data using `chunk()` or specifying different chunks in encoding."
     ]
    }
   ],
   "source": [
    "%time dsread.to_zarr('./dsread.zarr',  mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunks': (200, 100, 1), 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0), 'filters': None, '_FillValue': nan, 'dtype': dtype('float64')}\n",
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 200, lon: 100, time: 10000)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 0.0 0.2462 0.4925 0.7387 ... 48.26 48.51 48.75 49.0\n",
      "  * lon      (lon) float64 0.0 0.4949 0.9899 1.485 ... 47.52 48.01 48.51 49.0\n",
      "  * time     (time) datetime64[ns] 2001-01-01 ... 2002-02-21T15:00:00\n",
      "Data variables:\n",
      "    data     (lat, lon, time) float64 dask.array<shape=(200, 100, 10000), chunksize=(50, 25, 10000)>\n"
     ]
    }
   ],
   "source": [
    "print(dsread.data.encoding)\n",
    "print(dsread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Workaround: delete encoding before saving the xarray.Dataset as zarr. Do we have to do this each time? why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dsread.data.encoding['chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 10.4 s, total: 21.9 s\n",
      "Wall time: 12.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x374469da0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dsread.to_zarr('./dsread.zarr',  mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 200, lon: 100, time: 10000)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 0.0 0.2462 0.4925 0.7387 ... 48.26 48.51 48.75 49.0\n",
      "  * lon      (lon) float64 0.0 0.4949 0.9899 1.485 ... 47.52 48.01 48.51 49.0\n",
      "  * time     (time) datetime64[ns] 2001-01-01 ... 2002-02-21T15:00:00\n",
      "Data variables:\n",
      "    data     (lat, lon, time) float64 dask.array<shape=(200, 100, 10000), chunksize=(50, 25, 10000)>\n",
      "{'chunks': (50, 25, 10000), 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0), 'filters': None, '_FillValue': nan, 'dtype': dtype('float64')}\n"
     ]
    }
   ],
   "source": [
    "dsnew =xr.open_zarr('./dsread.zarr')\n",
    "print(dsnew)\n",
    "print(dsnew.data.encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
